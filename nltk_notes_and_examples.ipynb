{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to /home/juan/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'firefox.txt', u'grail.txt', u'overheard.txt', u'pirates.txt', u'singles.txt', u'wine.txt']\n"
     ]
    }
   ],
   "source": [
    "print webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \tPIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terry Rossio\n",
      "1: \t[view looking straight down at rolling swells, sound of wind and thunder, then a low heartbeat]\n",
      "2: \tScene: PORT ROYAL\n",
      "3: \t[teacups on a table in the rain]\n",
      "4: \t[sheet music on music stands in the rain]\n",
      "5: \t[bouquet of white orchids, Elizabeth sitting in the rain holding the bouquet]\n",
      "6: \t[men rowing, men on horseback, to the sound of thunder]\n",
      "7: \t[EITC logo on flag blowing in the wind]\n",
      "8: \t[many rowboats are entering the harbor]\n",
      "9: \t[Elizabeth sitting alone, at a distance]\n",
      "10: \t[marines running, kick a door in] \n"
     ]
    }
   ],
   "source": [
    "#Imprimimos las 10 primeras lineas del documento\n",
    "for i, line in enumerate(webtext.raw('pirates.txt').split(\"\\n\")):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print str(i) + \": \\t\" + str(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n"
     ]
    }
   ],
   "source": [
    "#Almacenamos la frase 8 en una variable.\n",
    "line_no8 = webtext.raw('singles.txt').split('\\n')[8]\n",
    "print line_no8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/juan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n",
       " u'Maybe we could explore new beginnings together?',\n",
       " u'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n",
       " u'You WONT be disappointed.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenización de la frase\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "sent_tokenize(line_no8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are\n",
      "you\n",
      "alone\n",
      "or\n",
      "lost\n",
      "in\n",
      "a\n",
      "r/ship\n",
      "too\n",
      ",\n",
      "with\n",
      "no\n",
      "hope\n",
      "in\n",
      "sight\n",
      "?\n",
      "maybe\n",
      "we\n",
      "could\n",
      "explore\n",
      "new\n",
      "beginnings\n",
      "together\n",
      "?\n",
      "im\n",
      "45\n",
      "slim/med\n",
      "build\n",
      ",\n",
      "gsoh\n",
      ",\n",
      "high\n",
      "needs\n",
      "and\n",
      "looking\n",
      "for\n",
      "someone\n",
      "similar\n",
      ".\n",
      "you\n",
      "wont\n",
      "be\n",
      "disappointed\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Lowercasing y tokenización. \n",
    "for sentence in sent_tokenize(line_no8):\n",
    "    for word in word_tokenize(sentence):\n",
    "        print word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Stopwords\n",
    "#nltk.download('book')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "stopwords_en = map(str, stopwords_en)\n",
    "print stopwords_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'you', 'wont', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tratamos las sentencias como un único documento (no es necesario hacer sent_tokenize)\n",
    "single_no8 = webtext.raw('singles.txt').split('\\n')[8]\n",
    "\n",
    "single_no8_tokenized = list(map(str.lower, word_tokenize(str(single_no8))))\n",
    "print single_no8_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alone', 'lost', 'r/ship', ',', 'hope', 'sight', '?', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'looking', 'someone', 'similar', '.', 'wont', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "#Eliminamos las stopwords de la frase \n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "not_in_stopwords = []\n",
    "for word in single_no8_tokenized:\n",
    "    if word not in stopwords_en:\n",
    "        not_in_stopwords.append(word)\n",
    "print not_in_stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('From string.punctuation:', <type 'str'>, '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n"
     ]
    }
   ],
   "source": [
    "#Eliminar los signos de puntuación\n",
    "from string import punctuation\n",
    "print('From string.punctuation:', type(punctuation), punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'all', u'just', u\"don't\", u'being', '-', u'over', u'through', u'during', u'its', u'before', '$', u'hadn', u'with', u'll', u'had', ',', u'should', u'to', u'only', u'does', u'under', u'ours', u'has', u\"should've\", u\"haven't\", u'do', u'them', u'his', u'very', u\"you've\", u'they', u'not', u'yourselves', u'now', u'him', u'nor', '`', u'd', u'did', '=', u'didn', '^', u'this', u'she', u'each', u\"won't\", u'needn', u'where', u\"mustn't\", u\"isn't\", u'because', u\"you'd\", u'doing', u'theirs', u'some', u'we', u'up', u'are', u'further', u'ourselves', u'out', u'what', u'for', u\"needn't\", '+', u'weren', '/', u're', u'won', u\"shouldn't\", u'above', u'between', u'mustn', '?', u't', u'be', u'hasn', u'after', u\"mightn't\", u\"you're\", u'here', u'shouldn', u'hers', '[', u\"aren't\", u'by', '_', u'both', u'about', u'couldn', u'of', u\"wouldn't\", '&', u'o', '|', u's', u'isn', '(', '{', '~', u'or', u'own', '*', u'into', u'while', u'yourself', u'down', u\"hadn't\", u'mightn', u\"couldn't\", u'wasn', u'your', u\"doesn't\", '\"', u'from', u'her', u'their', u'aren', u\"it's\", u'there', u'been', '.', u'few', u'too', u'then', u'themselves', ':', u'was', u'until', '\\\\', u'more', u'himself', u'on', u'am', u'but', ';', '@', u\"that'll\", u'herself', u'than', u'those', u'he', u'me', u'myself', u'ma', u\"wasn't\", u\"hasn't\", '<', u'will', u'below', u'ain', u'can', u'were', '>', u'my', u'at', u'and', u've', u'wouldn', u'is', u'in', u\"didn't\", u'it', u'doesn', u'an', u'as', u'itself', u'against', u'have', u'our', u\"shan't\", u'any', u'whom', '!', u'these', '%', u'no', ')', u'that', u'when', u'off', u'same', u'how', u'other', u'which', u'you', u'if', u'shan', u\"weren't\", u'haven', u'again', u'who', '#', u'most', u'such', ']', u'why', u'a', u'don', \"'\", u'i', u'm', u'having', u\"you'll\", u'so', u'y', u\"she's\", u'the', '}', u'yours', u'once'])\n"
     ]
    }
   ],
   "source": [
    "#Para eliminar los signos de puntuación, los unimos a la lista de stopwords.\n",
    "\n",
    "stopwords_en_withpunct = stopwords_en.union(set(punctuation))\n",
    "print(stopwords_en_withpunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alone', 'lost', 'r/ship', 'hope', 'sight', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'needs', 'looking', 'someone', 'similar', 'wont', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "#Eliminamos las stopwords y los signos de puntuación de la frase\n",
    "word_list = []\n",
    "for word in single_no8_tokenized:\n",
    "    if word not in stopwords_en_withpunct:\n",
    "        word_list.append(word)\n",
    "        \n",
    "print word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Podemos encontrar más listas de stopwords en otros idiomas. Vamos a usar alguna más.\n",
    "# Stopwords from stopwords-json\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "#Realizamos la unión de todas ellas\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lost', 'r/ship', 'hope', 'sight', 'explore', 'beginnings', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'similar', 'wont', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "#Volvemos a eliminar las stopwords de la frase, notaremos un resultado más preciso.\n",
    "final_word_list = []\n",
    "for word in single_no8_tokenized:\n",
    "    if word not in stoplist_combined:\n",
    "        final_word_list.append(word)\n",
    "print final_word_list        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "walk\n",
      "walk\n"
     ]
    }
   ],
   "source": [
    "#Stemming (reducción de la palabra a su raíz) y lemmatization (buscar la raiz)\n",
    "\n",
    "#Ejemplo de Stemming con PorterStemmer \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for word in ['walking','walks','walked']:\n",
    "    print porter.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n",
      "walk\n",
      "walked\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo de lemmatization con WordNetLemmatizer\n",
    "from nltk.ststoplist_combinedem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for word in ['walking','walks','walked']:\n",
    "    print wnl.lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRP'), ('is', 'VBZ'), ('walking', 'VBG'), ('to', 'TO'), ('school', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizar es un proceso más complejo y necesita 'Parts of speech' que no cubriremos de momento.\n",
    "#Por defecto, el WordNetLemmatizer asumirá siempre que las palabras son sustantivos\n",
    "#si no le especificamos lo contrario\n",
    "\n",
    "#Ejemplo\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' #Si no encuentra solución, deja la palabra como sustantivo.\n",
    "    \n",
    "walking_tagged = pos_tag(word_tokenize('He is walking to school'))\n",
    "print(walking_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', u'be', u'walk', 'to', 'school']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ahora trataremos de lemmatizar realmente las palabras de una frase.\n",
    "def lemmatize_sent(text): \n",
    "   \n",
    "    list = []\n",
    "    for word, tag in pos_tag(word_tokenize(text)):\n",
    "        list.append(wnl.lemmatize(word.lower(), pos=penn2morphy(tag)))\n",
    "\n",
    "    return list\n",
    "\n",
    "\n",
    "lemmatize_sent('He is walking to school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n"
     ]
    }
   ],
   "source": [
    "#Ahora vamos a lemmatizar y quitar las stopwords de la frase 8 original.\n",
    "print single_no8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lose', 'r/ship', 'hope', 'sight', 'explore', 'beginning', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'similar', 'wont', 'disappoint']\n"
     ]
    }
   ],
   "source": [
    "list = []\n",
    "for word in lemmatize_sent(single_no8):\n",
    "    if word not in stoplist_combined:\n",
    "        list.append(word)\n",
    "        list = map(str, list)\n",
    "print list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', u'jump', 'lazy', 'brown', 'dog']\n",
      "['mr', 'brown', u'jump', 'lazy', 'fox']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    proccesed_text = []\n",
    "    for word in lemmatize_sent(text):\n",
    "        if word not in stoplist_combined:\n",
    "            proccesed_text.append(word)\n",
    "    \n",
    "    return proccesed_text\n",
    "\n",
    "text1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "processed_sent1 = preprocess_text(text1)\n",
    "text2 = \"Mr brown jumps over the lazy fox.\"\n",
    "processed_sent2 = preprocess_text(text2)\n",
    "\n",
    "print processed_sent1\n",
    "print processed_sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'brown': 2, 'lazy': 1, 'fox': 1, 'dog': 1, u'jump': 1, 'quick': 1})\n",
      "Counter({u'jump': 1, 'brown': 1, 'lazy': 1, 'fox': 1, 'mr': 1})\n"
     ]
    }
   ],
   "source": [
    "#Contamos el numero de veces que aparece cada palabra en la frase.\n",
    "from collections import Counter\n",
    "\n",
    "print Counter(processed_sent1)\n",
    "print Counter(processed_sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'brown': 0, u'lazy': 4, u'over': 6, u'fox': 2, u'dog': 1, u'mr': 5, u'quick': 7, u'the': 8, u'jumps': 3}\n"
     ]
    }
   ],
   "source": [
    "#Vectorización. Ponemos las palabras y cuentas en una tabla. Usaremos SKLearn.\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#El valor original debe ser unicode, por lo que añadiremos la u antes del string\n",
    "sent1 = u\"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = u\"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
    "    # Create the vectorizer\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit_transform(fin)\n",
    "    \n",
    "    print count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'brown': 0,\n",
       " u'dog': 1,\n",
       " u'fox': 2,\n",
       " u'jumps': 3,\n",
       " u'lazy': 4,\n",
       " u'mr': 5,\n",
       " u'quick': 6}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No hemos especificado al CountVectorizer que quite la puntuación, ni que tokenize ni que convierta a minusculas\n",
    "#Pero lo ha hecho, debido a la configuracion por defecto del algoritmo\n",
    "'''\n",
    "CountVectorizer(\n",
    "    input=’content’, encoding=’utf-8’, \n",
    "    decode_error=’strict’, strip_accents=None, \n",
    "    lowercase=True, preprocessor=None, \n",
    "    tokenizer=None, stop_words=None, \n",
    "    token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), \n",
    "    analyzer=’word’, max_df=1.0, min_df=1, \n",
    "    max_features=None, vocabulary=None, \n",
    "    binary=False, dtype=<class ‘numpy.int64’>)[source]\n",
    "'''\n",
    "#Podemos sobreescribir estos parámetros con funciones que ya hemos creado, como la lista de stopwords.\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sent1 = u\"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = u\"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
    "    # Override the analyzer totally with our preprocess text\n",
    "    count_vect = CountVectorizer(stop_words=stoplist_combined,\n",
    "                                 tokenizer=word_tokenize)\n",
    "    count_vect.fit_transform(fin)\n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
